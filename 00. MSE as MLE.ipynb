{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE as Maximum Likelihood\n",
    "- MSE(mean-squared error) is a commonly used error metric. But is it principly justified?\n",
    "- We show that minimising MSE is equivalent to maximising the likelihood on a linear Gaussian model.\n",
    "Assume the data is described by the linear model $\\mathbf{y} = \\mathbf{wX}+ \\epsilon$, where $\\epsilon_i \\sim N(\\epsilon_i; 0,\\sigma^2_e)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=attachment:image.png width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Likelihood is the probability of the data given the parameters of the model, in this case, $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The log likelihood of our model is\n",
    "$$\\log p(\\mathbf{y}|\\mathbf{X, w}) = \\sum_{i=1}^N \\log p(y_i | \\mathbf{x_i, \\theta\n",
    "\n",
    "But since the noise $\\epsilon$ is Gaussian (i.e. normally distributed), the likelihood is just\n",
    "\n",
    "$$ \\begin{aligned}  \\log p(\\mathbf{y}|\\mathbf{X, w}) &= \\sum_{i=1}^N \\log N(y_i;\\mathbf{x_iw},\\sigma^2) \\\\  &= \\sum_{i=1}^N \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2_e}}\\exp (-\\frac{(y_i - \\mathbf{x_iw})^2}{2\\sigma^2_e}) \\\\  &= -\\frac{N}{2}\\log 2\\pi\\sigma^2_e - \\sum_{i=1}^N \\frac{(y_i-\\mathbf{x_iw)^2}}{2\\sigma^2_e}  \\end{aligned}$$\n",
    "\n",
    "where N is the number of datapoints. So, \n",
    "\n",
    "$$\\begin{aligned}  \\mathbf{w}_{MLE} &= \\arg\\max_{\\mathbf{w}} - \\sum_{i=1}^N (y_i-\\mathbf{x_iw})^2 \\\\  &= \\arg\\min_{\\mathbf{w}} \\frac{1}{N}\\sum_{i=1}^N (y_i-\\mathbf{x_iw})^2 \\\\  \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood as minimising KL Divergence\n",
    "- KL Divergence measures the dissimilarity between two probability distributions:\n",
    "\n",
    "$$D_{KL}(p||q) = E_{\\mathbf{x}\\sim p(\\mathbf{x})}[\\log p(\\mathbf{x})-\\log q(\\mathbf{x})]$$ \n",
    "\n",
    "- minimize KL divergence between the empirical distribution $\\hat{p}_{\\text{data}}$ and the model distribution $p_{\\text{model}}$\n",
    "\n",
    "$$\\begin{aligned}  \\theta_{\\text{min KL}} &= \\arg\\min_{\\theta} D_{KL}(\\hat{p}_{\\text{data}} || p_{\\text{model}}) \\\\  &= \\arg\\min_{\\theta} E_{\\mathbf{x}\\sim{\\hat{p}_{\\text{data}}}}[\\log \\hat{p}_\\text{data}(\\mathbf{x})-\\log p_{\\text{model}}(\\mathbf{x})]  \\end{aligned}$$\n",
    "\n",
    "- But, $E_{\\mathbf{x}\\sim{\\hat{p}_{\\text{data}}}}\\log \\hat{p}_\\text{data}(\\mathbf{x})$ is independent of the model parameters $\\theta$, so:\n",
    "\n",
    "$$\\begin{aligned}  \\theta_{\\text{min KL}} &= \\arg\\min_{\\theta} - E_{\\mathbf{x}\\sim{\\hat{p}_{\\text{data}}}}[\\log p_{\\text{model}}(\\mathbf{x}|\\theta)]  \\end{aligned} $$\n",
    "\n",
    "- We can turn this negative argmin to an argmax and if the datapoints x are i.i.d., we have:\n",
    "$$\\begin{aligned}  \\theta_{\\text{min KL}}&= \\arg\\max_{\\theta} \\lim\\limits_{N\\to\\infty}\\frac{1}{N}\\sum_{i=1}^N\\log(p(\\mathbf{x_i}|\\theta)) \\\\  &= \\theta_{MLE}  \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnd",
   "language": "python",
   "name": "rnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
