{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout as a Bayesian Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's consider a network with a single hidden layer and the task of regression. A standard network would output $\\widehat{y} = \\sigma(x W_1 + b) W_2$.\n",
    "- To use dropout we sample two binary vectors $z_1, z_2$. The elements of vector $z_i$ take value 1 with probability $0 \\le p_i \\le 1$ for $i = 1,2$. Given an input $x$, we set $1 - p_1$ of the elements of the input to zero: $x z_1$. The output of the first layer is given by $\\sigma(x z_1 W_1 + b)$, in which we randomly set $1 - p_2$ proportion of the elements to zero, and linearly transform to give the dropout model's output $\\widehat{y} = \\sigma(x z_1 W_1 + b) z_2 W_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the network for regression we might use the euclidean loss, $ E = \\frac{1}{2N} \\sum_{n=1}^N ||y_n - \\widehat{y}_n||^2_2 $ During optimisation a regularisation term is often added. We often use $L_2$ regularisation weighted by some weight decay $\\lambda_{decay}$, resulting in a minimisation objective, \\begin{align*} \\label{eq:L:dropout} L_{\\text{dropout}} := E + \\lambda_{decay} \\big( &||W_1||^2_2 + ||W_2||^2_2 \\notag\\\\ &+ ||b||^2_2 \\big). \\end{align*} We sample new realisations for the binary vectors $z_i$ for every input point and every forward pass thorough the model, and use the same values in the backward pass. The dropped weights $z_1W_1$ and $z_2W_2$ are scaled by $\\frac{1}{p_i}$ to maintain constant output magnitude. At test time we simply use the full weights matrices $W_1,W_2,b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modelling our distribution over the space of functions with a Gaussian process we can analytically evaluate its corresponding posterior in regression tasks, and estimate the posterior in classification tasks. In practice what this means is that for regression we place a joint Gaussian distribution over all function values, \\begin{align*} \\label{eq:generative_model_reg} F \\vert X &\\sim N(z, K(X, X)) \\\\ Y \\vert F &\\sim N(F, \\tau^{-1} I_N) \\notag \\end{align*} with some precision hyper-parameter $\\tau$ and where $I_N$ is the identity matrix with dimensions $N \\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimising the KL divergence is the same as maximising the log evidence lower bound with respect to $\\theta$: \\begin{align}\\label{ELBO} L_{\\text{VI}} := \\int q_\\theta(w) \\log p(Y | X, w) d w - KL(q_\\theta(w) || p(w)) \\end{align} \n",
    "\n",
    "We are given a GP kernel function of the form \\begin{align*} K(x, y) = \\int N(w; 0,l^{-2}I_Q) p(b) \\sigma(w^T x + b) \\sigma(w^T y + b) d w d b \\end{align*} with prior length-scale $l$, some distribution $p(b)$ and $\\sigma$ an element-wise non-linear function (e.g. ReLU/TanH).\n",
    "\n",
    "We approximate this kernel function with Monte Carlo integration with $K$ terms: \\begin{align*} \\hat{K}(x, y) &= \\frac{1}{K} \\sum_{k=1}^K \\sigma(w_k^T x + b_k) \\sigma(w_k^T y + b_k) \\end{align*} with $w_k \\sim N(0,l^{-2}I_Q)$ and $b_k \\sim p(b)$. This is a random kernel function. The $K$ terms in the Monte Carlo integration would correspond to $K$ hidden units in our network as we'll see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use a variational distribution $q_\\theta(w)=q_\\theta(W_1)q_\\theta(W_2)q_\\theta(b)$ to approximate the posterior $p(w | X, Y)$: \\begin{align*} q_\\theta(W_1) = \\prod_{q=1}^Q q_\\theta(w_q), ~~~ q_\\theta(w_q) = p_1 N(m_q, \\sigma^2 I_K) + (1-p_1) N(0, \\sigma^2 I_K) \\notag \\end{align*} \n",
    "Defining the approximating distribution as a mixture of Gaussians with a small enough standard deviation (e.g. machine epsilon) results in a finite KL divergence where the term that would have diverged to infinity now takes a constant value.\n",
    "\n",
    "We approximate the log evidence lower bound with Monte Carlo integration with a single sample $\\widehat{w} \\sim q_\\theta(w)$: \\begin{align*} L_{\\text{GP-MC}} \\approx \\log p(Y | X, \\widehat{w}) - \\frac{p_1 l^2}{2} ||M_1||^2_2 - \\frac{p_2 l^2}{2} ||M_2||^2_2 - \\frac{l^2}{2}||m||^2_2, \\end{align*} where we approximated the second term following a theorem brought in the appendix of (Gal and Ghahramani). This is an unbiased estimator of $L_{\\text{VI}}$.\n",
    "\n",
    "Scaling the objective by the constant $\\frac{1}{N \\tau}$, we get the maximisation objective: \\begin{align*} L_{\\text{GP-MC}} &\\propto - \\frac{1}{2 N} \\sum_{n=1}^N || y_n - \\widehat{y}_n ||^2_2 - \\frac{p_1 l^2}{2 N \\tau} ||M_1||^2_2 - \\frac{p_2 l^2}{2 N \\tau} ||M_2||^2_2 - \\frac{l^2}{2 N \\tau}||m||^2_2 \\end{align*} recovering the dropout objective with appropriate model precision $\\tau$ and prior length-scale $l$ for small enough $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a great sense of similarity between the first equation for dropout NN optimisation and the second one for GP with approximating distribution $q(\\omega)$ s.t. $\\omega = \\{diag(\\epsilon_1)M_1, diag(\\epsilon_2)M_2, b\\}$ with $\\epsilon_l \\sim Bernoulli(1-p_l) (l = 1, 2)$ (which we will refer to as a Bernoulli variational distribution or a dropout variational distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate, decay):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.decay = decay\n",
    "        self.f = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1,20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_rate),\n",
    "            torch.nn.Linear(20, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=self.dropout_rate),\n",
    "            torch.nn.Linear(20,1)\n",
    "        )\n",
    "    def forward(self, X):        \n",
    "        return self.f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainity_estimate(x, model, num_samples, l2):\n",
    "    outputs = np.hstack([model(x).cpu().detach().numpy() for i in range(num_samples)]) # në²ˆ inference, output.shape = [20, N]\n",
    "    y_mean = outputs.mean(axis=1)\n",
    "    y_variance = outputs.var(axis=1)\n",
    "    tau = l2 * (1. - model.dropout_rate) / (2. * N * model.decay)\n",
    "    y_variance += (1. / tau)\n",
    "    y_std = np.sqrt(y_variance)\n",
    "    return y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200 ##  number of points\n",
    "min_value = -10\n",
    "max_value = 10\n",
    "\n",
    "\n",
    "x_obs = np.linspace(min_value, max_value, N)\n",
    "noise = np.random.normal(loc = 10, scale = 80, size = N)\n",
    "y_obs =  x_obs**3 + noise\n",
    "\n",
    "x_test = np.linspace(min_value - 10, max_value + 10, N)\n",
    "y_test = x_test**3 + noise\n",
    "\n",
    "# Normalise data:\n",
    "x_mean, x_std = x_obs.mean(), x_obs.std()\n",
    "y_mean, y_std = y_obs.mean(), y_obs.std()\n",
    "x_obs = (x_obs - x_mean) / x_std\n",
    "y_obs = (y_obs - y_mean) / y_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "y_test = (y_test - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x_obs, y_obs)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(dropout_rate=0.5, decay=1e-6).to(device)\n",
    "criterion  = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=model.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(20000):\n",
    "    y_pred = model(torch.Tensor(x_obs).view(-1,1).to(device))\n",
    "    y_true = Variable(torch.Tensor(y_obs).view(-1,1).to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iter % 2000 == 0:\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "y_pred = model(torch.Tensor(x_obs).view(-1,1).to(device))\n",
    "plt.plot(x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"observed\")\n",
    "plt.plot(x_obs, y_pred.cpu().detach().numpy(), ls=\"-\", color=\"b\", label=\"mean\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_uncertainty = 200\n",
    "\n",
    "lengthscale = 0.01\n",
    "n_std = 2 # number of standard deviations to plot\n",
    "y_mean, y_std = uncertainity_estimate(torch.Tensor(x_test).view(-1,1).to(device), model, iters_uncertainty, lengthscale)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x_obs, y_obs, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"observed\")\n",
    "plt.plot(x_test, y_mean, ls=\"-\", color=\"b\", label=\"mean\")\n",
    "plt.plot(x_test, y_test, ls='--', color='r', label='true')\n",
    "for i in range(n_std):\n",
    "    plt.fill_between( x_test,\n",
    "        y_mean - y_std * ((i+1.)),\n",
    "        y_mean + y_std * ((i+1.)),\n",
    "        color=\"b\",\n",
    "        alpha=0.1)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC-Dropout with MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), batch_size=64, shuffle=True, num_workers=4)\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])), batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(F.dropout(self.conv1(x), training=True), 2))\n",
    "        x = F.relu(F.max_pool2d(F.dropout(self.conv2(x), training=True), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=True)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):        \n",
    "        data, target = data.cuda(), target.cuda()    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 300 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] \\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(data),\n",
    "                          len(train_loader.dataset),\n",
    "                          100. * batch_idx / len(train_loader),\n",
    "                          loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target)  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcdropout_test(model):\n",
    "    model.train()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    T = 50\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output_list = []\n",
    "        for i in range(T):\n",
    "            output_list.append(torch.unsqueeze(model(data), 0))\n",
    "        output_mean = torch.cat(output_list, 0).mean(0)\n",
    "        test_loss += criterion(output_mean, target)  # sum up batch loss\n",
    "        pred = output_mean.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nMC Dropout Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdropout_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 60\n",
    "rotation_matrix = torch.Tensor([[[math.cos(r/360.0*2*math.pi), -math.sin(r/360.0*2*math.pi), 0],\n",
    "                                        [math.sin(r/360.0*2*math.pi), math.cos(r/360.0*2*math.pi), 0]]]).to(device)\n",
    "print(rotation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_test(model):\n",
    "    model.train()\n",
    "    T = 50\n",
    "    rotation_list = range(0, 180, 10)\n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output_list = []\n",
    "        image_list = []\n",
    "        unct_list = []\n",
    "        for r in rotation_list:\n",
    "            rotation_matrix = Variable(torch.Tensor([[[math.cos(r/360.0*2*math.pi), -math.sin(r/360.0*2*math.pi), 0],\n",
    "                                                    [math.sin(r/360.0*2*math.pi), math.cos(r/360.0*2*math.pi), 0]]]).cuda(),\n",
    "                                       volatile=True)\n",
    "            grid = F.affine_grid(rotation_matrix, data.size())\n",
    "            data_rotate = F.grid_sample(data, grid)\n",
    "            image_list.append(data_rotate)\n",
    "\n",
    "            for i in range(T):\n",
    "                output_list.append(torch.unsqueeze(F.softmax(model(data_rotate)), 0))\n",
    "            output_mean = torch.cat(output_list, 0).mean(0)\n",
    "            output_variance = torch.cat(output_list, 0).var(0).mean().item()\n",
    "            confidence = output_mean.data.cpu().numpy().max()\n",
    "            predict = output_mean.data.cpu().numpy().argmax()\n",
    "            unct_list.append(output_variance)\n",
    "            print ('rotation degree', str(r).ljust(3), 'Uncertainty : {:.4f} Predict : {} Softmax : {:.2f}'.format(output_variance, predict, confidence))\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(len(rotation_list)):\n",
    "            ax = plt.subplot(2, len(rotation_list)/2, i+1)\n",
    "            plt.text(0.5, -0.5, \"{0:.3f}\".format(unct_list[i]),\n",
    "                     size=12, ha=\"center\", transform=ax.transAxes)\n",
    "            plt.axis('off')\n",
    "            plt.gca().set_title(str(rotation_list[i])+u'\\xb0')\n",
    "            plt.imshow(image_list[i][0, 0, :, :].data.cpu().numpy())\n",
    "        plt.show()\n",
    "        if idx > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
